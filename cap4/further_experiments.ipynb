{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "41aab30e",
   "metadata": {},
   "source": [
    "## 4.1.6 further experiments"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4da2e463",
   "metadata": {},
   "source": [
    "The following experiments will help convince you that the architecture choices you’ve made are all fairly reasonable, although there’s still room for improvement:\n",
    "\n",
    "a)You used two representation layers before the final classification layer. Try using one or three representation layers, and see how doing so affects validation and test accuracy.\n",
    "\n",
    "b)Try using layers with more units or fewer units: 32 units, 64 units, and so on. \n",
    "\n",
    "\n",
    "c)Try using the mse loss function instead of binary_crossentropy. \n",
    "\n",
    "\n",
    "d)Try using the tanh activation (an activation that was popular in the early days of neural networks) instead of relu."
   ]
  }
 ],
 "metadata": {
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
