{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "b77c5d49",
   "metadata": {},
   "source": [
    "### This chapter covers:\n",
    "- Understanding the tension between generalization and optimization, the fundamental issue in machine learning Evaluation methods for machine learning models\n",
    "- Best practices to improve model fitting\n",
    "- Best practices to achieve better generalization"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "36afc162",
   "metadata": {},
   "source": [
    "### Weight regularization"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a73e70ed",
   "metadata": {},
   "source": [
    "In Keras, weight regularization is added by passing weight regularizer instances to layers as keyword arguments. Let’s add L2 weight regularization to our initial movie-review classification model."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "3af56fa8",
   "metadata": {},
   "outputs": [],
   "source": [
    "from keras import regularizers, layers\n",
    "from tensorflow import keras\n",
    "\n",
    "model = keras.Sequential([\n",
    "    layers.Dense(16,\n",
    "                kernel_regularizer=regularizers.l2(0.002),\n",
    "                activation=\"relu\"),\n",
    "    layers.Dense(16,\n",
    "                kernel_regularizer=regularizers.l2(0.002)),\n",
    "    layers.Dense(1, activation=\"sigmoid\")\n",
    "])\n",
    "\n",
    "model.compile(optimizer=\"rmsprop\",\n",
    "            loss=\"binary_crossentropy\",\n",
    "            metrics=[\"accuracy\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "38b77569",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/20\n",
      "\u001b[1m30/30\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m4s\u001b[0m 39ms/step - accuracy: 0.7882 - loss: 0.5884 - val_accuracy: 0.8566 - val_loss: 0.4681\n",
      "Epoch 2/20\n",
      "\u001b[1m30/30\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 12ms/step - accuracy: 0.8919 - loss: 0.3937 - val_accuracy: 0.8685 - val_loss: 0.4089\n",
      "Epoch 3/20\n",
      "\u001b[1m30/30\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 13ms/step - accuracy: 0.9117 - loss: 0.3270 - val_accuracy: 0.8707 - val_loss: 0.3963\n",
      "Epoch 4/20\n",
      "\u001b[1m30/30\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 22ms/step - accuracy: 0.9271 - loss: 0.2889 - val_accuracy: 0.8860 - val_loss: 0.3668\n",
      "Epoch 5/20\n",
      "\u001b[1m30/30\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 23ms/step - accuracy: 0.9325 - loss: 0.2714 - val_accuracy: 0.8885 - val_loss: 0.3619\n",
      "Epoch 6/20\n",
      "\u001b[1m30/30\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 16ms/step - accuracy: 0.9372 - loss: 0.2568 - val_accuracy: 0.8794 - val_loss: 0.3846\n",
      "Epoch 7/20\n",
      "\u001b[1m30/30\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 22ms/step - accuracy: 0.9463 - loss: 0.2420 - val_accuracy: 0.8794 - val_loss: 0.3926\n",
      "Epoch 8/20\n",
      "\u001b[1m30/30\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 23ms/step - accuracy: 0.9487 - loss: 0.2353 - val_accuracy: 0.8828 - val_loss: 0.3818\n",
      "Epoch 9/20\n",
      "\u001b[1m30/30\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 25ms/step - accuracy: 0.9528 - loss: 0.2248 - val_accuracy: 0.8601 - val_loss: 0.4552\n",
      "Epoch 10/20\n",
      "\u001b[1m30/30\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 24ms/step - accuracy: 0.9539 - loss: 0.2221 - val_accuracy: 0.8781 - val_loss: 0.3975\n",
      "Epoch 11/20\n",
      "\u001b[1m30/30\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 23ms/step - accuracy: 0.9549 - loss: 0.2168 - val_accuracy: 0.8789 - val_loss: 0.4035\n",
      "Epoch 12/20\n",
      "\u001b[1m30/30\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 26ms/step - accuracy: 0.9588 - loss: 0.2103 - val_accuracy: 0.8710 - val_loss: 0.4414\n",
      "Epoch 13/20\n",
      "\u001b[1m30/30\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 23ms/step - accuracy: 0.9528 - loss: 0.2138 - val_accuracy: 0.8665 - val_loss: 0.4580\n",
      "Epoch 14/20\n",
      "\u001b[1m30/30\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 27ms/step - accuracy: 0.9607 - loss: 0.1981 - val_accuracy: 0.8724 - val_loss: 0.4346\n",
      "Epoch 15/20\n",
      "\u001b[1m30/30\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 28ms/step - accuracy: 0.9597 - loss: 0.2018 - val_accuracy: 0.8768 - val_loss: 0.4284\n",
      "Epoch 16/20\n",
      "\u001b[1m30/30\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 27ms/step - accuracy: 0.9610 - loss: 0.1966 - val_accuracy: 0.8652 - val_loss: 0.4858\n",
      "Epoch 17/20\n",
      "\u001b[1m30/30\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 27ms/step - accuracy: 0.9604 - loss: 0.1934 - val_accuracy: 0.8506 - val_loss: 0.5629\n",
      "Epoch 18/20\n",
      "\u001b[1m30/30\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 24ms/step - accuracy: 0.9653 - loss: 0.1858 - val_accuracy: 0.8706 - val_loss: 0.4665\n",
      "Epoch 19/20\n",
      "\u001b[1m30/30\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 26ms/step - accuracy: 0.9629 - loss: 0.1893 - val_accuracy: 0.8730 - val_loss: 0.4578\n",
      "Epoch 20/20\n",
      "\u001b[1m30/30\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 22ms/step - accuracy: 0.9670 - loss: 0.1828 - val_accuracy: 0.8625 - val_loss: 0.5184\n"
     ]
    }
   ],
   "source": [
    "from keras.datasets import imdb\n",
    "import numpy as np\n",
    "\n",
    "# Load the data\n",
    "(train_data, train_labels), (test_data, test_labels) = imdb.load_data(num_words=10000)\n",
    "\n",
    "# Vectorize the sequences (convert to multi-hot encoding)\n",
    "def vectorize_sequences(sequences, dimension=10000):\n",
    "    results = np.zeros((len(sequences), dimension))\n",
    "    for i, sequence in enumerate(sequences):\n",
    "        for j in sequence:\n",
    "            if j < dimension:  # Only encode words in our vocabulary\n",
    "                results[i, j] = 1\n",
    "    return results\n",
    "\n",
    "# Vectorize the data\n",
    "x_train = vectorize_sequences(train_data)\n",
    "x_test = vectorize_sequences(test_data)\n",
    "\n",
    "# Convert labels to float32\n",
    "y_train = np.asarray(train_labels).astype(\"float32\")\n",
    "y_test = np.asarray(test_labels).astype(\"float32\")\n",
    "\n",
    "# Now fit the model with vectorized data\n",
    "history_l2_reg = model.fit(x_train, y_train,\n",
    "    epochs=20, batch_size=512, validation_split=0.4)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "abc46ff0",
   "metadata": {},
   "source": [
    "As an alternative to L2 regularization, you can use one of the following Keras weight regularizers."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "84202dcc",
   "metadata": {},
   "source": [
    "### Dropout"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a0b3bd1a",
   "metadata": {},
   "source": [
    "In Keras, you can introduce dropout in a model via the Dropout layer, which is applied to the output of the layer right before it. Let’s add two Dropout layers in the IMDB model to see how well they do at reducing overfitting."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e48f0b23",
   "metadata": {},
   "outputs": [],
   "source": [
    "model = keras.Sequential([\n",
    "    layers.Dense(16, activation=\"relu\"),\n",
    "    layers.Dropout(0.5),\n",
    "    layers.Dense(16, activation=\"relu\"),\n",
    "    layers.Dropout(0.5),\n",
    "    layers.Dense(1, activation=\"sigmoid\")\n",
    "])\n",
    "\n",
    "model.compile(optimizer=\"rmsprop\",\n",
    "            loss=\"binary_crossentropy\",\n",
    "            metrics=[\"accuracy\"])\n",
    "\n",
    "history_dropout = model.fit(\n",
    "    x_train, y_train,\n",
    "    epochs=20, batch_size=512, validation_split=0.4)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "env",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.13.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
