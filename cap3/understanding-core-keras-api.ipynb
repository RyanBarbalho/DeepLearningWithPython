{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "a95d9c73",
   "metadata": {},
   "source": [
    "### 3.6.1 Layers: The building blocks of deep learning\n",
    "\n",
    "A layer is a data processing module that takes as input one or more tensors and that outputs one or more tensors. Some layers are stateless, but more frequently layers have a state: the layer’s weights, one or several tensors learned with stochastic gradient descent, which together contain the network’s knowledge."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6e76ec6f",
   "metadata": {},
   "source": [
    "### A dense layer implemented as a layer subclass"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "a6207bcd",
   "metadata": {},
   "outputs": [],
   "source": [
    "import tensorflow as tf\n",
    "from tensorflow import keras\n",
    "\n",
    "class SimpleDense(keras.layers.Layer): #all keras layers inherit from the base layer class\n",
    "    def __init__(self, units, activation=None):\n",
    "        super().__init__()\n",
    "        self.units = units\n",
    "        self.activation = activation\n",
    "\n",
    "    def build(self, input_shape):#weight creation takes placce in build()\n",
    "        input_dim = input_shape[-1]\n",
    "\n",
    "        #add_weight = atalho do keras cria variaveis standalone como atributos da layer\n",
    "        #ex: self.W = tf.Variable(tf.random.uniform(w_shape))\n",
    "        self.W = self.add_weight(shape=(input_dim, self.units),\n",
    "                                initializer=\"random_normal\")\n",
    "        self.b = self.add_weight(shape=(self.units,),\n",
    "                                initializer=\"zeros\")\n",
    "\n",
    "    def call(self, inputs):\n",
    "        y = tf.matmul(inputs,self.W) + self.b\n",
    "        if self.activation is not None:\n",
    "            y = self.activation(y)\n",
    "        return y"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "78172a57",
   "metadata": {},
   "source": [
    "Once instantiated, a layer like this can be used just like a function, taking as input a TensorFlow tensor:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "84a8bf93",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(2, 32)\n"
     ]
    }
   ],
   "source": [
    "#instancia a layer\n",
    "my_dense = SimpleDense(units=32, activation=tf.nn.relu)\n",
    "input_tensor = tf.ones(shape=(2, 784))\n",
    "output_tensor = my_dense(input_tensor)\n",
    "print(output_tensor.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5a527839",
   "metadata": {},
   "source": [
    "very layer will only accept input tensors of a certain shape and will return output tensors of a certain shape. Consider the following example:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "12e6d1be",
   "metadata": {},
   "outputs": [],
   "source": [
    "from keras import layers\n",
    "layer = layers.Dense(32, activation=\"relu\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "90adc9a3",
   "metadata": {},
   "source": [
    "This layer will return a tensor where the first dimension has been transformed to be 32. It can only be connected to a downstream layer that expects 32-dimensional vectors as its input"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4d5e69a4",
   "metadata": {},
   "outputs": [],
   "source": [
    "from keras import models,layers\n",
    "\n",
    "model = models.Sequential([\n",
    "    layers.Dense(32, activation=\"relu\"),\n",
    "    layers.Dense(32)\n",
    "])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6fed9a86",
   "metadata": {},
   "source": [
    "The layers didn’t receive any information about the shape of their inputs—instead, they automatically inferred their input shape as being the shape of the first inputs they see. \n",
    "\n",
    "In the toy version of the Dense layer we implemented in chapter 2 (which we named NaiveDense), we had to pass the layer’s input size explicitly to the constructor in order to be able to create its weights. That’s not ideal, because it would lead to models that look like this, where each new layer needs to be made aware of the shape of the layer before it:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "408cc7c4",
   "metadata": {},
   "outputs": [],
   "source": [
    "model = NaiveSequential([\n",
    "    NaiveDense(input_size=784, output_size=32, activation=\"relu\"),\n",
    "    NaiveDense(input_size=32, output_size=64, activation=\"relu\"),\n",
    "    NaiveDense(input_size=64, output_size=32, activation=\"relu\"),\n",
    "    NaiveDense(input_size=32, output_size=10, activation=\"softmax\")\n",
    "])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8646ea1a",
   "metadata": {},
   "source": [
    "## For now, just remember: when implementing your own layers, put the forward pass in the call() method."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8b18243a",
   "metadata": {},
   "source": [
    "- Two-branch networks \n",
    "- Multihead networks \n",
    "- Residual connections"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f51d7dd8",
   "metadata": {},
   "source": [
    "ere are generally two ways of building such models in Keras: you could directly subclass the Model class, or you could use the Functional API, which lets you do more with less code. We’ll cover both approaches in chapter 7."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ebcddf4a",
   "metadata": {},
   "source": [
    "By choosing a network topology, you constrain your space of possibilities (hypothesis space) to a specific series of tensor operations, mapping input data to output data. What you’ll then be searching for is a good set of values for the weight tensors involved in these tensor operations."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6e1d0022",
   "metadata": {},
   "source": [
    "To learn from data, you have to make assumptions about it. These assumptions define what can be learned. As such, the structure of your hypothesis space—the architecture of your model—is extremely important. It encodes the assumptions you make about your problem, the prior knowledge that the model starts with.\n",
    "\n",
    "two class classification -> sei que o problema é linearmente separável"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3b50d25e",
   "metadata": {},
   "source": [
    "### 3.6.3 The “compile” step: Configuring the learning process"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cbf93572",
   "metadata": {},
   "source": [
    "apos a arquitetura, precisamos escolher:"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4a0a1af5",
   "metadata": {},
   "source": [
    "- Loss function (objective function)—The quantity that will be minimized during training. It represents a measure of success for the task at hand.\n",
    "\n",
    "- Optimizer—Determines how the network will be updated based on the loss function. It implements a specific variant of stochastic gradient descent (SGD).\n",
    "\n",
    "- Metrics—The measures of success you want to monitor during training and validation, such as classification accuracy. Unlike the loss, training will not optimize directly for these metrics. As such, metrics don’t need to be differentiable.'"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5aaa48b1",
   "metadata": {},
   "source": [
    "### compile & fit"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1c32f733",
   "metadata": {},
   "source": [
    "The compile() method configures the training process"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "615ac3a3",
   "metadata": {},
   "outputs": [],
   "source": [
    "from keras.src.optimizers import optimizer\n",
    "\n",
    "\n",
    "model = keras.Sequential([keras.layers.Dense(1)]) #classificador linear\n",
    "#specify optimizer, loss, list of metrics\n",
    "model.compile(optimizer=\"rmsprop\",\n",
    "            loss=\"mean_squared_error\",\n",
    "            metrics=[\"accuracy\"])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f868e7c1",
   "metadata": {},
   "source": [
    "it’s also possible to specify these arguments as object instances"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "97ef76c2",
   "metadata": {},
   "source": [
    "if you want to pass your own custom losses or metrics, or if you want to further configure the objects you’re using—for instance, by passing a learning_rate argument to the optimizer:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fe27693b",
   "metadata": {},
   "outputs": [],
   "source": [
    "model.compile(optimizer=keras.optimizers.RMSprop(learning_rate=1e-4),\n",
    "            loss=\"mean_squared_error\",\n",
    "            metrics=[\"accuracy\"])"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "env",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.13.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
