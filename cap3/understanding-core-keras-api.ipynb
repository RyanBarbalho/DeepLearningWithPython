{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "a95d9c73",
   "metadata": {},
   "source": [
    "### 3.6.1 Layers: The building blocks of deep learning\n",
    "\n",
    "A layer is a data processing module that takes as input one or more tensors and that outputs one or more tensors. Some layers are stateless, but more frequently layers have a state: the layer’s weights, one or several tensors learned with stochastic gradient descent, which together contain the network’s knowledge."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6e76ec6f",
   "metadata": {},
   "source": [
    "### A dense layer implemented as a layer subclass"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "a6207bcd",
   "metadata": {},
   "outputs": [],
   "source": [
    "import tensorflow as tf\n",
    "from tensorflow import keras\n",
    "\n",
    "class SimpleDense(keras.layers.Layer): #all keras layers inherit from the base layer class\n",
    "    def __init__(self, units, activation=None):\n",
    "        super().__init__()\n",
    "        self.units = units\n",
    "        self.activation = activation\n",
    "\n",
    "    def build(self, input_shape):#weight creation takes placce in build()\n",
    "        input_dim = input_shape[-1]\n",
    "\n",
    "        #add_weight = atalho do keras cria variaveis standalone como atributos da layer\n",
    "        #ex: self.W = tf.Variable(tf.random.uniform(w_shape))\n",
    "        self.W = self.add_weight(shape=(input_dim, self.units),\n",
    "                                initializer=\"random_normal\")\n",
    "        self.b = self.add_weight(shape=(self.units,),\n",
    "                                initializer=\"zeros\")\n",
    "\n",
    "    def call(self, inputs):\n",
    "        y = tf.matmul(inputs,self.W) + self.b\n",
    "        if self.activation is not None:\n",
    "            y = self.activation(y)\n",
    "        return y"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "78172a57",
   "metadata": {},
   "source": [
    "Once instantiated, a layer like this can be used just like a function, taking as input a TensorFlow tensor:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "84a8bf93",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(2, 32)\n"
     ]
    }
   ],
   "source": [
    "#instancia a layer\n",
    "my_dense = SimpleDense(units=32, activation=tf.nn.relu)\n",
    "input_tensor = tf.ones(shape=(2, 784))\n",
    "output_tensor = my_dense(input_tensor)\n",
    "print(output_tensor.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5a527839",
   "metadata": {},
   "source": [
    "very layer will only accept input tensors of a certain shape and will return output tensors of a certain shape. Consider the following example:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "12e6d1be",
   "metadata": {},
   "outputs": [],
   "source": [
    "from keras import layers\n",
    "layer = layers.Dense(32, activation=\"relu\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "90adc9a3",
   "metadata": {},
   "source": [
    "This layer will return a tensor where the first dimension has been transformed to be 32. It can only be connected to a downstream layer that expects 32-dimensional vectors as its input"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4d5e69a4",
   "metadata": {},
   "outputs": [],
   "source": [
    "from keras import models,layers\n",
    "\n",
    "model = models.Sequential([\n",
    "    layers.Dense(32, activation=\"relu\"),\n",
    "    layers.Dense(32)\n",
    "])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6fed9a86",
   "metadata": {},
   "source": [
    "The layers didn’t receive any information about the shape of their inputs—instead, they automatically inferred their input shape as being the shape of the first inputs they see. \n",
    "\n",
    "In the toy version of the Dense layer we implemented in chapter 2 (which we named NaiveDense), we had to pass the layer’s input size explicitly to the constructor in order to be able to create its weights. That’s not ideal, because it would lead to models that look like this, where each new layer needs to be made aware of the shape of the layer before it:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "408cc7c4",
   "metadata": {},
   "outputs": [],
   "source": [
    "model = NaiveSequential([\n",
    "    NaiveDense(input_size=784, output_size=32, activation=\"relu\"),\n",
    "    NaiveDense(input_size=32, output_size=64, activation=\"relu\"),\n",
    "    NaiveDense(input_size=64, output_size=32, activation=\"relu\"),\n",
    "    NaiveDense(input_size=32, output_size=10, activation=\"softmax\")\n",
    "])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8646ea1a",
   "metadata": {},
   "source": [
    "## For now, just remember: when implementing your own layers, put the forward pass in the call() method."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8b18243a",
   "metadata": {},
   "source": [
    "- Two-branch networks \n",
    "- Multihead networks \n",
    "- Residual connections"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f51d7dd8",
   "metadata": {},
   "source": [
    "ere are generally two ways of building such models in Keras: you could directly subclass the Model class, or you could use the Functional API, which lets you do more with less code. We’ll cover both approaches in chapter 7."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ebcddf4a",
   "metadata": {},
   "source": [
    "By choosing a network topology, you constrain your space of possibilities (hypothesis space) to a specific series of tensor operations, mapping input data to output data. What you’ll then be searching for is a good set of values for the weight tensors involved in these tensor operations."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6e1d0022",
   "metadata": {},
   "source": [
    "To learn from data, you have to make assumptions about it. These assumptions define what can be learned. As such, the structure of your hypothesis space—the architecture of your model—is extremely important. It encodes the assumptions you make about your problem, the prior knowledge that the model starts with.\n",
    "\n",
    "two class classification -> sei que o problema é linearmente separável"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3b50d25e",
   "metadata": {},
   "source": [
    "### 3.6.3 The “compile” step: Configuring the learning process"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cbf93572",
   "metadata": {},
   "source": [
    "apos a arquitetura, precisamos escolher:"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4a0a1af5",
   "metadata": {},
   "source": [
    "- Loss function (objective function)—The quantity that will be minimized during training. It represents a measure of success for the task at hand.\n",
    "\n",
    "- Optimizer—Determines how the network will be updated based on the loss function. It implements a specific variant of stochastic gradient descent (SGD).\n",
    "\n",
    "- Metrics—The measures of success you want to monitor during training and validation, such as classification accuracy. Unlike the loss, training will not optimize directly for these metrics. As such, metrics don’t need to be differentiable.'"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5aaa48b1",
   "metadata": {},
   "source": [
    "### compile & fit"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1c32f733",
   "metadata": {},
   "source": [
    "The compile() method configures the training process"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "615ac3a3",
   "metadata": {},
   "outputs": [],
   "source": [
    "from keras.src.optimizers import optimizer\n",
    "\n",
    "\n",
    "model = keras.Sequential([keras.layers.Dense(1)]) #classificador linear\n",
    "#specify optimizer, loss, list of metrics\n",
    "model.compile(optimizer=\"rmsprop\",\n",
    "            loss=\"mean_squared_error\",\n",
    "            metrics=[\"accuracy\"])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f868e7c1",
   "metadata": {},
   "source": [
    "it’s also possible to specify these arguments as object instances"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "97ef76c2",
   "metadata": {},
   "source": [
    "if you want to pass your own custom losses or metrics, or if you want to further configure the objects you’re using—for instance, by passing a learning_rate argument to the optimizer:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fe27693b",
   "metadata": {},
   "outputs": [],
   "source": [
    "model.compile(optimizer=keras.optimizers.RMSprop(learning_rate=1e-4),\n",
    "            loss=\"mean_squared_error\",\n",
    "            metrics=[\"accuracy\"])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "119589cc",
   "metadata": {},
   "source": [
    "### 3.6.4 Picking a loss function"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "48b90f91",
   "metadata": {},
   "source": [
    "For instance, you’ll use binary crossentropy for a two-class classification problem, categorical crossentropy for a many-class classification problem, and so on. Only when you’re working on truly new research problems will you have to develop your own loss functions."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b121c790",
   "metadata": {},
   "source": [
    "### 3.6.5 Understanding the fit() method"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6c9c0770",
   "metadata": {},
   "source": [
    "The fit() method implements the training loop itself. These are its key arguments:"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0022f5a7",
   "metadata": {},
   "source": [
    "- The data (inputs and targets) to train on. It will typically be passed either in the form of NumPy arrays or a TensorFlow Dataset object.\n",
    "\n",
    "- The number of epochs to train for: how many times the training loop should iterate over the data passed.\n",
    "\n",
    "- The batch size to use within each epoch of mini-batch gradient descent: the number of training examples considered to compute the gradients for one weight update step."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "867ee02d",
   "metadata": {},
   "outputs": [],
   "source": [
    "history = model.fit(\n",
    "    inputs, #inputs(numpy array)\n",
    "    targets, #targets de treinamento, numpyarray\n",
    "    epochs=5, #training loop itera 5 vezes\n",
    "    batch_size=128 #training loop ira iterar em lotes de 128 exemplos\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "abe447a9",
   "metadata": {},
   "source": [
    "The call to fit() returns a History object. This object contains a history field, which is a dict mapping keys such as \"loss\" or specific metric names to the list of their perepoch values."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ae5974c9",
   "metadata": {},
   "outputs": [],
   "source": [
    "history.history \n",
    "{\"binary_accuracy\": [0.855, 0.9565, 0.9555, 0.95, 0.951], \n",
    "\"loss\": [0.6573270302042366,\n",
    "    0.07434618508815766,\n",
    "    0.07687718723714351,  \n",
    "    0.07412414988875389, \n",
    "    0.07617757616937161]}"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5e0f9a45",
   "metadata": {},
   "source": [
    "The goal of machine learning is to obtain models that perform well in general, and particularly on data points that the model has never encountered before."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bcdefcc0",
   "metadata": {},
   "source": [
    "### usando o argumento validation_data"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5b26091b",
   "metadata": {},
   "source": [
    "you track how the model is performing on new data by using the validation_data argument in fit(). Like the training data, the validation data could be passed as NumPy arrays or as a TensorFlow Dataset object."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c6f57c48",
   "metadata": {},
   "outputs": [],
   "source": [
    "model = keras.Sequential([keras.layers.Dense(1)])\n",
    "model.compile(optimizer=keras.optimizers.RMSprop(learning_rate=0.1),\n",
    "            loss=keras.losses.MeanSquaredError(),\n",
    "            metrics=[keras.metrics.BinaryAccuracy()])\n",
    "\n",
    "#to avoid having samples from only one class in validation data,\n",
    "#shuffle the inputs and targets using a random indices permutation\n",
    "\n",
    "indices_permutation = np.random.permutation(len(inputs))\n",
    "shuffled_inputs = inputs[indices_permutation]\n",
    "shuffled_targets = targets[indices_permutation]\n",
    "\n",
    "# reserve 30% of the training inputs e targets for validation\n",
    "#(exclude these samples from training and reserve them to compute\n",
    "#the validation loss and metrics)\n",
    "\n",
    "num_validation_samples = int(0.3 * len(inputs))\n",
    "val_inputs = shuffled_inputs[:num_validation_samples] #0 ->30%\n",
    "val_targets = shuffled_targets[:num_validation_samples]\n",
    "training_inputs = shuffled_inputs[num_validation_samples:]\n",
    "training_targets = shuffled_targets[num_validation_samples:]\n",
    "\n",
    "#validation data será usada para monitorar as metricas e loss\n",
    "model.fit(\n",
    "    training_inputs,\n",
    "    training_targets,\n",
    "    epochs=5,\n",
    "    batch_size=16,\n",
    "    validation_data=(val_inputs, val_targets)\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "eca8f206",
   "metadata": {},
   "source": [
    "Note that if you want to compute the validation loss and metrics after the training is complete, you can call the evaluate() method:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cae29cf8",
   "metadata": {},
   "outputs": [],
   "source": [
    "loss_and_metrics = model.evaluate(val_inputs, val_targets, batch_size=128)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ce412d87",
   "metadata": {},
   "source": [
    "evaluate() will iterate in batches (of size batch_size) over the data passed and return a list of scalars, where the first entry is the validation loss and the following entries are the validation metrics. If the model has no metrics, only the validation loss is returned (rather than a list)."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a45a5f4c",
   "metadata": {},
   "source": [
    "### 3.6.7 Inference: Using a model after training"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "47a3c26c",
   "metadata": {},
   "source": [
    "Once you’ve trained your model, you’re going to want to use it to make predictions on new data. \n",
    "\n",
    "This is called inference: To do this, a naive approach would simply be to __call__() the model:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ed354636",
   "metadata": {},
   "outputs": [],
   "source": [
    "predictions = model(new_inputs)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e3f0f205",
   "metadata": {},
   "source": [
    "esse processo pode ser muito contexto dependendo do tamanho dos dados de input. A better way to do inference is to use the predict() method. It will iterate over the data in small batches and return a NumPy array of predictions. And unlike __call__(), it can also process TensorFlow Dataset objects"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d746efe3",
   "metadata": {},
   "outputs": [],
   "source": [
    "predictions = model.predict(new_inputs, batch_size=128)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d5f8dbac",
   "metadata": {},
   "source": [
    "if we use predict() on some of our validation data with the linear model we trained earlier, we get scalar scores that correspond to the model’s prediction for each input sample:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "35729081",
   "metadata": {},
   "outputs": [],
   "source": [
    ">>> predictions = model.predict(val_inputs, batch_size=128)\n",
    ">>> print(predictions[:10]) \n",
    "[[0.3590725 ] \n",
    "  [0.82706255]\n",
    "  [0.74428225]\n",
    "  [0.682058 ] \n",
    "  [0.7312616 ] \n",
    "  [0.6059811 ] \n",
    "  [0.78046083] \n",
    "  [0.025846 ] \n",
    "  [0.16594526] \n",
    "  [0.72068727]]"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "env",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.13.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
